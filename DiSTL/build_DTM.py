from DiSTL.cleaning_util import vocab_cleaner
from pymongo import MongoClient
from datetime import datetime
from dask import delayed
import dask.dataframe as dd
import pandas as pd
import numpy as np
import logging
import shutil
import glob
import dask
import imp
import csv
import sys
import os
import re


"""
Code for building DTM from token database, currently this just support
Mongodb but CSV files and SQL are in the works.

At a high level the way this works:

    1. The config file contains several aggregation pipelines, as well
    as key cleaning params.

        1.a search_pipeline

        returns a subset of documents matching the search criteria

        1.b count_pipeline

        returns a count for the number of articles produced by the
        search pipeline

        1.c vocab_pipeline

        returns a set of terms and any correpsonding counts, applied
        after the search_pipeline

        1.d dtm_pipeline

        returns document term pairs with their corresponding counts
        to build sparse DTM representation

        1.e vocab_cleaning_params

        A dictionary which includes all the arguments passed to
        the vocab cleaning method

    2. The run process is then structured as follows:

        2.a.i build counts
        2.a.ii build vocab
        2.a.iii build DTM

        For each stage, there are two steps

        2.b.i Run build_DTM with stage specific key-words
        2.b.ii Run stage specific cleaning scripts

        build_DTM works by accessing the database with many parallel
        instances, running any search code to subset the doucments, and
        then storing temporary files containing the results from each
        parallel query.

        The cleaning code then loads these temporary files and aggregates
        them up to the stage specific values and does any necessary cleaning

        2.c.i clean_counts

        Just aggregates the counts as no cleaning is required

        2.c.ii clean_vocab

        Takes the results from build_vocab, and applies all the desired
        cleaning rules.  The result of this is two maps

            - raw_term -> term_id
            - clean_term -> term_id

        These maps can then be applied by build_triplet to introduce any
        cleaning desired.

        2.c.iii clean_triplet

        Aggregates the temporary files generated by build_triplet and produces
        a doc_id which aligns with all partitions of data.

"""


# ----------------- #
# Token DB Backends #
# ----------------- #

def _mongodb_backend(collection, db, pipeline):
    """returns a generator for the specified aggregate pipeline over
    the collection and db

    Parameters
    ----------
    collection : iterable
        collection name for current partition
    db : str
        name of database
    pipeline : list of dictionaries
        aggregation pipeline for generator

    Returns
    -------
    generator of documents
    """

    client = MongoClient()
    client.admin.command({"setParameter": 1, "cursorTimeoutMillis": 60000000})
    generator = client[db][collection].aggregate(pipeline, allowDiskUse=True)
    return generator


def _csv_backend(partition, filters=None):
    """returns a generator for the specified filter set over the specified
    csv file (containing tokens)

    Parameters
    ----------
    partition : str
        label for current partition
    filters : dict-like or None
        filters applied to csv to select docs

    Returns
    -------
    generator of documents
    """

    return None


def _sql_backend(partition):
    """returns a generator for the specified SQL db

    Parameters
    ----------
    partition : str
        label for current partition

    Returns
    -------
    generator of documents
    """

    return None


# -------- #
# Builders #
# -------- #

def default_builder(doc_generator):
    """
    builds a DF that contains whatever the generator returns
    """

    df = pd.DataFrame([r for r in doc_generator])
    return df,


def count_builder(doc_generator):
    """
    builds a DF containing one entry for the count, the doc_generator here
    should yield one entry identifying the number of observations
    """

    count = pd.DataFrame([{"count": r["count"]} for r in doc_generator])
    return count,


def vocab_builder(doc_generator):
    """
    builds a DF where each entry corresponds to a term and its count.  The
    generator should yield one observation for each term.  Note that this
    is all handled in the generator so just calls _default_builder
    """

    return _default_builder(doc_generator)


def DTM_builder(doc_generator, term_id_map):
    """
    builds a pair of DFs containing doc ids as well as triplet representation
    of DTM

    Parameters
    ----------
    term_id_map : dict-like
        map from raw_term -> term_id.  If a term doesn't end up in the vocab
        it shouldn't be in this dict
    """

    doc_id_l = []
    DTM_l = []

    for doc_id, doc in enumerate(doc_generator):

        metadata = doc["_id"]
        metadata["doc_id"] = doc_id
        doc_id_l.append(metadata)

        term_dict_l = doc["txt"]
        for term_dict in term_dict_l:
            # We do try except here because it seems to be quicker
            # than checking for the existence of a term in term_id_map
            # first.  There may be a better way to do this...
            term = term_dict["term"]
            try:
                term_id = term_id_map[term]
                count = term_dict["count"]
                triplet = {"term_id": term_id, "doc_id": doc_id,
                           "count": count}
                DTM_l.append(triplet)
            except:
                continue
    doc_id = pd.DataFrame(doc_id_l)
    DTM = pd.DataFrame(DTM_l)

    return doc_id, DTM


# ----------------------- #
# Partition Build Wrapper #
# ----------------------- #

def partition_handler(partition, backend, builder, backend_kwds,
                      builder_kwds, tmp_file_name_patterns,
                      log_file, log_info):
    """
    builds a tuple of dataframes for each parittion and then writes to the
    temporary files for cleaning/aggregation.

    Parameters
    ----------
    partition : str
        label for current partition
    backend : str
        label corresponding to backend
    builder : function
        method for processing generator, returns a tuple of dataframes
    backend_kwds : dict-like
        key-words passed to backend
    builder_kwds : dict-like
        key-words passed to builder
    tmp_file_name_patterns : iterable
        tuple of patterns corresponding to location of tmp files used to store
        intermediary results.  Note that these should be patterns allowing
        for insertion of partition label.
    log_file : str or None
        location of log file if used (logs how long this takes)
    log_info : str or None
        label info to include in log output

    Returns
    -------
    None
    """

    # TODO handle tmp file tuples better
    # TODO handle tmp file pattern interaction with partitions better

    t0 = datetime.now()

    if backend == "mongodb":
        doc_generator = _mongodb_backend(partition, **backend_kwds)
    else:
        raise ValueError("backend: %s, is currently not supported" % backend)

    df_tup = builder(doc_generator, **builder_kwds)

    for tf, df in zip(tmp_file_name_patterns, df_tup):
        df.to_csv(tf % partition, index=False)

    if log_file:
        t1 = datetime.now()
        with open(log_file, "a") as f:
            f.write("%s %s %s\n" % (log_info, partition, str(t1 - t0)))


def build_wrapper(partitions, backend, builder, tmp_file_name_patterns,
                  backend_kwds=None, builder_kwds=None,
                  log_file=None, log_info=None):
    """
    runs the partition handler in parallel over the different partitions

    Parameters
    ----------
    partitions : iterable
        list of partition labels
    backend : str
        label corresponding to backend
    backend_kwds : dict-like or None
        key-words passed to backend
    builder : function
        method for processing generator, returns a tuple of dataframes
    tmp_file_name_patterns : iterable or None
        tuple of patterns corresponding to location of tmp files used to store
        intermediary results.  Note that these should be patterns allowing
        for insertion of partition label.
    backend_kwds : dict-like or None
        key-words passed to backend
    builder_kwds : dict-like or None
        key-words passed to builder
    log_file : str or None
        location of log file if used (logs how long this takes)
    log_info : str or None
        label info to include in log output

    Returns
    -------
    None
    """

    # TODO remove tmp files and build dataframes directory from delayed
    # NOTE, the only reason this is hard right now is because each delayed
    # object returns a tuple of dataframes instead of a single data frame

    if backend_kwds is None:
        backend_kwds = {}
    if builder_kwds is None:
        builder_kwds = {}

    t0 = datetime.now()

    del_l = [delayed(partition_handler)(part, backend, builder,
                                        backend_kwds, builder_kwds,
                                        tmp_file_name_patterns,
                                        log_file, log_info)
             for part in partitions]

    dask.compute(*del_l)

    if log_file:
        t1 = datetime.now()
        with open(log_file, "a") as f:
            f.write("%s all %s\n" % (log_info, str(t1 - t0)))


# -------------------- #
# Cleaning/Aggregation #
# -------------------- #

def clean_count(data_dir, log=True):
    """
    very simple, just aggregates all the tmp count files into one file

    Parameters
    ----------
    data_dir : str
        location where data files are stored
    log : bool
        indicator for whether to record a log entry when done

    Returns
    -------
    None
    """

    t0 = datetime.now()

    # load and aggregate vocab
    tmp_count_files = os.path.join(data_dir, "tmp_count_*.csv")
    count = dd.read_csv(tmp_count_files).compute()

    # write the results
    count.to_csv(os.path.join(data_dir, "count.csv"), index=False)

    # remove temporary files
    for f in glob.glob(tmp_count_files):
        os.remove(os.path.join(data_dir, f))

    if log:
        t1 = datetime.now()
        with open(os.path.join(data_dir, "DTM.log"), "a") as f:
            f.write("cc %s\n" % str(t1 - t0))


def clean_vocab(data_dir, stop_words=None, regex_stop_words=None,
                D=None, doc_lthresh=None, doc_uthresh=None,
                tfidf_thresh=None, stemmer=None,
                term_length=None, log=True):
    """
    loads the temporay vocab files, returned from each process,
    and combines these into one vocab by summing the counts.
    Any desired cleaning rules and then a term id is generated
    for each remaining term.

    The resulting maps from

    raw_term -> term_id
    clean_term -> term_id

    are then written to the data directory.

    Parameters
    ----------
    data_dir : str
        location where data files are stored
    stop_words : list or None
        list of words which should be removed from vocab
    regex_stop_words : list or None
        list of regex patterns which should be removed from vocab
    stemmer : function or None
        function applied to pandas Series to generated stemmed
        version of series
    doc_<lthresh|uthresh> : scalar
        <lower|upper> threshold for stemming/stop word doc count thresholding
    D : int or None
        D is the number of documents in the corpus if D is > 0 we assume
        the doc_<X> thresh is between 0 and 1 so we multiply by D, otherwise
        we use the threshold as-is.
    tfidf_thresh : scalar
        tfidf threshold below for stemming/stop words thresholding.
    term_length : scalar
        minimum length required for term
    log : bool
        indicator for whether to record a log entry when done

    Returns
    -------
    None
    """

    t0 = datetime.now()

    # load count to get doc count
    count = pd.read_csv(os.path.join(data_dir, "count.csv"))
    D = count["count"].sum()

    # load and aggregate vocab
    tmp_vocab_files = os.path.join(data_dir, "tmp_vocab_*.csv")
    vocab = dd.read_csv(tmp_vocab_files)
    vocab = vocab.groupby("_id").sum().compute()
    vocab["raw_term"] = vocab.index
    vocab = vocab.reset_index(drop=True)

    # now apply cleaning rules
    vocab = vocab_cleaner(vocab, stop_words, regex_stop_words, doc_lthresh,
                          doc_uthresh, tfidf_thresh, stemmer, term_length)

    # now generate term_id maps
    vocab = vocab.reset_index(drop=True)
    vocab["term_id"] = vocab.index

    # write the results
    vocab.to_csv(os.path.join(data_dir, "vocab.csv"), index=False)

    # remove temporary files
    for f in glob.glob(tmp_vocab_files):
        os.remove(os.path.join(data_dir, f))

    if log:
        t1 = datetime.now()
        with open(os.path.join(data_dir, "DTM.log"), "a") as f:
            f.write("vc %s\n" % str(t1 - t0))


def clean_triplet(data_dir, log=True):
    """
    loads the temporary DTM files generated by build_triplet, the doc_ids are
    combined into one large doc_id index.  This index is mapped to the DTM
    and the resulting files are written to the data directory

    Parameters
    ----------
    data_dir : str
        location where data files are stored
    log : bool
        indicator for whether to record a log entry when done

    Returns
    -------
    None
    """

    t0 = datetime.now()

    # load tmp files
    tmp_doc_files = os.path.join(data_dir, "tmp_doc_id_*.csv")
    doc_id_dd = dd.read_csv(tmp_doc_files, blocksize=None)

    tmp_dtm_files = os.path.join(data_dir, "tmp_DTM_*.csv")
    dtm_dd = dd.read_csv(tmp_dtm_files, blocksize=None)

    # generate correct doc_id
    doc_id_dd["new_doc_id"] = 1
    doc_id_dd["new_doc_id"] = doc_id_dd["new_doc_id"].cumsum()

    # now map new id to old id
    delayed_dtm = dtm_dd.to_delayed()
    delayed_doc_id = doc_id_dd.to_delayed()
    dtm_l = len(delayed_dtm)
    doc_l = len(delayed_doc_id)
    if dtm_l != doc_l:
        raise ValueError("doc_id and DTM partitions aren't same length")

    def zip_mapper(dtm_i, doc_id_i):

        doc_id_i.index = doc_id_i["doc_id"]
        id_dict = doc_id_i["new_doc_id"]
        dtm_i["doc_id"] = dtm_i["doc_id"].map(id_dict)
        return dtm_i

    del_l = [delayed(zip_mapper)(dtm_i, doc_i)
             for dtm_i, doc_i in zip(delayed_dtm, delayed_doc_id)]
    dtm_dd = dd.from_delayed(del_l)
    doc_id_dd["doc_id"] = doc_id_dd["new_doc_id"]
    doc_id_dd = doc_id_dd.drop("new_doc_id", axis=1)

    # write results to data file
    dtm_dd.to_csv(os.path.join(data_dir, "DTM_*.csv"), index=False)
    doc_id_dd.to_csv(os.path.join(data_dir, "doc_id_*.csv"), index=False)

    # remove tmp files
#    tmp_doc_files = glob.glob(tmp_doc_files)
#    for f in tmp_doc_files:
#        os.remove(f)
#    tmp_dtm_files = glob.glob(tmp_dtm_files)
#    for f in tmp_dtm_files:
#        os.remove(f)

    if log:
        t1 = datetime.now()
        with open(os.path.join(data_dir, "DTM.log"), "a") as f:
            f.write("dc %s\n" % str(t1 - t0))
